% !TeX spellcheck = it_IT
% --------------------------------------------------------------------
% Preamble
% --------------------------------------------------------------------
\documentclass[paper=a4, fontsize=11pt,twoside]{scrartcl}   % KOMA

\usepackage[a4paper,pdftex]{geometry}   % A4paper margins
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,fit,matrix,positioning}
\tikzset
{
	treenode/.style = {circle, draw=black, align=center, minimum size=1cm},
	subtree/.style  = {isosceles triangle, draw=black, align=center, minimum height=0.5cm, minimum width=1cm, shape border rotate=90, anchor=north}
}

\setlength{\oddsidemargin}{5mm}         % Remove 'twosided' indentation
\setlength{\evensidemargin}{5mm}
\setlength{\parskip}{1mm}

\usepackage[italian]{babel}
\usepackage[protrusion=true,expansion=true]{microtype}  
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{graphicx}

% --------------------------------------------------------------------
% Definitions (do not change this)
% --------------------------------------------------------------------
\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}   % Horizontal rule

\makeatletter                           % Title
\def\printtitle{%                       
	{\centering \@title\par}}
\makeatother                                    

\makeatletter                           % Author
\def\printauthor{%                  
	{\centering \large \@author}}               
\makeatother                            

% --------------------------------------------------------------------
% Metadata (Change this)
% --------------------------------------------------------------------
\title{ \normalsize \textsc{Relazione progetto Algoritmi e Strutture Dati}    % Subtitle
	\\[2.0cm]                               % 2cm spacing
	\HRule{0.5pt} \\                        % Upper rule
	\LARGE \textbf{\uppercase{Implementazione dizionario in linguaggio 
			C con Red-Black Tree}}    % Title
	\HRule{2pt} \\ [0.5cm]      % Lower rule + 0.5cm spacing
	\normalsize 31 Maggio 2017          % Todays date
}

\author{
	Nicolas Farabegoli\\
	Paolo Baldini\\
	Università di Bologna\\  
	\texttt{nicolas.farabegoli@studio.unibo.it} \\
	\texttt{paolo.baldini@studio.unibo.it}\\
}



\begin{document}
	% ------------------------------------------------------------------------------
	% Maketitle
	% ------------------------------------------------------------------------------
	\thispagestyle{empty}       % Remove page numbering on this page
	
	\printtitle                 % Print the title data as defined above
	\vfill
	\printauthor                % Print the author data as defined above
	\newpage
	\tableofcontents
	\newpage
	% ------------------------------------------------------------------------------
	% Begin document
	% ------------------------------------------------------------------------------
	\setcounter{page}{1}        % Set page numbering to begin on this page
	
	\section{Requisiti progetto}
		L'obiettivo del progetto è quello di costruire e gestire un vocabolario 
		contenente le voci, ordinate, e le corrispondenti definizioni. 
		Il dizionario può essere inizialmente generato a partire da un file di testo.  
		In questo caso si vogliono memorizzare tutte le parole presenti in un file di 
		testo in ordine lessico grafico.
		
		Si utilizzi per rappresentare il dizionario la struttura dati che si ritiene 
		più opportuna, dopo una attenta analisi. Si possono utilizzare le strutture 
		dati astratte affrontate dal corso (alberi, hash table, code, pile, liste, 
		grafi, ... ).
		
		Il file contiene un testo le cui parole non sono memorizzate in ordine lessicografico, e sono separate da spazio. I vocaboli sono inizialmente salvati senza la corrispondente definizione; la definizione, una stringa di dimensione sconosciuta, sarà inserita in un secondo momento, tramite la funzione apposita.
		
		Ogni voce del dizionario contiene al massimo 20 caratteri, e minimo 2, e le definizioni inserite ne contengono al massimo 50.
		
			\subsection{Vincoli}
				Nel dizionario non devono essere inserite le parole di un solo carattere o con più di 10 caratteri. Inoltre non devono essere inserite parole che differiscono solo per caratteri minuscoli/minuscoli. Le parole e le definizioni inserite nel dizionario non devono presentare caratteri maiuscoli; nel caso di inserimento di una parola con un carattere, o più, maiuscoli, convertirli per inserirli nel modo corretto. 
				
				Si assume che nel file di testo e nel dizionario non sono presenti nomi propri (di persona) e le voci nel dizionario possono ammettere caratteri accentati (è, à, etc...), ma non ammettono cifre e caratteri di punteggiatura.
			
			\subsection{Funzionalità richieste}
				Sono presenti metodi di gestione del dizionario come il conteggio delle voci del dizionario, l'inserimento o la cancellazione di un nuovo vocabolo, ...				
				Esistono due diverse funzioni di ricerca:
				\begin{itemize}
					\item la prima, di base, dato un termine restituisce, se presente, la sua definizione
					\item la seconda è una funzione di searchAdvance che verifica la presenza della parola da cercare e restituisce la tre parole più simile a quella che si vuole cercare. Per similarità si intende il minor numero di modifiche (sostituzioni, inserimenti) per trasformare una parola nell'altra.
				\end{itemize} \pagebreak
				 Le specifiche della ricerca avanzata, e l'algoritmo da utilizzare, sono a scelta dello studente. Inoltre si vuole avere la possibilità di caricare e salvare il dizionario, a partire dalla struttura dati ottenuta, con il seguente formato, in un file testuale (\textbf{.txt}):
				 \begin{trivlist}
				 	\item \texttt{"che": [(null)]}
				 	\item \texttt{"classe: [(null)]}
				 	\item \texttt{"come": [nc]}
				 	\item \texttt{"determinato": [ulz u]}
				 	\item \texttt{"di": [(null)]}
				 \end{trivlist}
			 	 Oltre al salvataggio classico è richiesto di sviluppare il salvataggio del file compresso, quindi con un ridotto quantitativo di spazio in memoria, utilizzando la tecnica di compressione di Huffman. Il vocabolario è compresso a partire dallo stesso formato del normale salvataggio ("che": [(null)] ...). 
			 	 
			 	 Oltre al salvataggio del file compresso, si vuole avere la possibilità di caricare il dizionario compresso da un file di test (decompressione), in modo tale da poterlo visualizzare e/o rielaborare successivamente. Il testo ottenuto dalla decompressione è dello stresso formato del dizionario salvato in memoria. La funzione deve produrre in uscita la struttura dati vocabolario scelta.
			 	 \newpage
		\section{Aspetti del progetto}
			\subsection{Componenti del gruppo}
				Il gruppo si compone di due persone: Paolo Baldini (\texttt{xxxxxxx}), Nicolas Farabegoli (\texttt{788928}).
			\subsection{Suddivisione del lavoro}
				Sono state assegnate 13 funzioni per lavorare sulla struttura dati; l'implementazione delle suddette funzioni è stato ripartita nel seguente modo:
				\begin{itemize}
					\item \texttt{createFormFile()} - Nicolas Farabegoli
					\item \texttt{printDictionary()} - Paolo Baldini
					\item \texttt{countWord()} - Paolo Baldini
					\item \texttt{insertWord()} - Nicolas Farabegoli
					\item \texttt{cancWord()} - Nicolas Farabegoli
					\item \texttt{getWordAt()} - Paolo Baldini
					\item \texttt{insertDef()} - Nicolas Farabegoli
					\item \texttt{searchDef()} - Nicolas Farabegoli
					\item \texttt{saveDictionary()} - Nicolas Farabegoli
					\item \texttt{importDictionary()} - Nicolas Farabegoli
					\item \texttt{searchAdvance()} - Paolo Baldini
					\item \texttt{compressHuffman()} - Paolo Baldini
					\item \texttt{decompressHuffman()} - Paolo Baldini			
				\end{itemize}
			 
			 \subsection{Tools di sviluppo}
			 	I tools di sviluppo che sono stati utilizzati sono: Visual Studio Enterprise 2015 (Editor + compilatore) per effettuare il test in ambiente Windows, mentre VIM (Editor) e GCC (compilatore) per il test in ambiente Linux.
			 	
			 	I test del software in ambiente Windows sono stati effettuati su Windows 10 Pro, invece i test in ambiente linux sono stati effettuati utilizzando Arch Linux, Kernel 4.11.3-1 e GCC 7.1.1.
			 \subsection{Deadline}
			 	Il progetto viene terminato per il primo appello di Giugno, si stima quindi come data di consegna del progetto (in accordo con le scadenze imposte dal docente) il \today.
			 	
		\section{Analisi del progetto}
			\subsection{Scelta struttura dati}
				 A seguito delle specifiche del progetto ci è stato richiesto di effettuare un attenta analisi sulla struttura dati da utilizzare per realizzare il dizionario. In un primo momento si era pensato di adottare come struttura dati una tabella Hash; questa però è risultata sin da subito non efficiente all'implementazione di un dizionario: come prima cosa avremo dovuto creare una funzione Hash ad hoc estremamente efficiente e che gestisse le \textit{collision} in maniera intelligente, non ci è sembrata una struttura dati adatta al contesto del progetto proprio per il fatto che le tabelle Hash generano collision. Una possibile soluzione pensata per gestire le collision era quella di creare una lista in corrispondenza di una collision. Un'approfondita analisi circa il problema delle collision ci ha portati a riflettere sui costi computazionali: gestendo le collision con una lista, il \textit{worst case} sia dell'inserimento che della ricerca nel dizionario, avrebbero avuto un costo pari a $\Theta(n)$.
				 
				 Questo lo si può facilmente intuire dal fatto che: supponendo di avere una funzione Hash che crei un numero di collision molto grande in un unico punto, troveremo la lista corrispondente a quel punto con lunghezza (in termini di ordini di grandezza) pari a $n$. Quindi sia l'inserimento (in ordine lessicografico) che la ricerca, hanno come costo computazionale pari a $\Theta(n)$.\\\\
				 Ci siamo quindi concentrati su strutture dati che abbiano costi computazionali al più lineari $\Theta(n)$. Ci siamo quindi orientati sugli alberi binari di ricerca (BST).				 
				 Abbiamo quindi iniziato a stimare i costi computazionali per le funzioni di maggiore rilievo sulla struttura dati, come inserimento, cancellazione, ricerca e visita. Dalle nostre stime è emerso che: per \textit{inserimento}, \textit{cancellazione} e \textit{ricerca} il costo computazionale è $\Theta(log(n))$ (grazie alle proprietà che caratterizzano i BST), mentre per la \textit{visita} il costo è $\Theta(n)$.
				 
				 Un'ulteriore analisi sulla struttura dati ci ha condotti ad osservare che gli alberi binari di ricerca non erano ancora la soluzione ottimale per il problema: infatti i costi stimati in precedenza fanno riferimento al caso in cui l'albero binario sia bilanciato. Infatti se ciò non fosse, i costi che prima erano stimati a $\Theta(log(n))$, nel caso di un albero fortemente sbilanciato sono $\Theta(n)$. Ritornando alle problematiche precedentemente riscontrate con le tabelle Hash. Non potendo prevedere a posteriori il bilanciamento dell'albero e considerando sempre il caso peggiore, abbiamo appurato che i BST non sono ancora la struttura dati ottimale per implementare un dizionario.\\\\
				 La chiave per ottenere una struttura dati che ci consenta di effettuare le principali operazione in tempo sub-lineare e al più lineare è quella di riuscire a bilanciare l'albero.
				 Abbiamo quindi pensato ai Red-Black Tree (RBT), i quali ci garantiscono un buon bilanciamento dell'albero, con conseguente garanzia sui costi computazionali stimati nei casi peggiori. Infatti nel \textit{worst case} abbiamo 
				 per \textit{inserimento}, \textit{cancellazione} e \textit{ricerca} un costo pari a $\Theta(log(n))$, mentre per la \textit{visita} rimane sempre $\Theta(n)$. La struttura dati utilizzata nel progetto è il \textbf{Red-Black Tree}.
				 
			\subsection{RBT e BST a confronto}
				Come accennato alla sezione precedente, i BST sono fortemente inclini a sbilanciarsi, ciò causa un'alterazione nei costi computazionali a tal punto che l'ordine di grandezza si sposta da sub-lineare a lineare. Questo è un problema da tenere in considerazione quando si lavora con un numero di elementi molto elevato come può essere un dizionario.
				Il problema del bilanciamento è praticamente risolto adottando i Red-Black Tree, poiché la politica di inserimento e cancellazione fa si che l'albero sia fortemente bilanciato, riuscendo, nel caso peggiore, ad avere un costo computazionale pari a $\Theta(2log(n))$.
			
			\subsection{Stime costi computazionali: algoritmi a confronti}				
				\begin{table}[ht]	
					\centering									
					\begin{tabular}{l | l | l}
						
						Algorithm & Average case & Worst Case \\
						\hline
						Hash Table & $\Theta(xxx)$ & $\Theta(xxx)$ \\
						\hline
						BST & $\Theta(log(n))$ & $\Theta(n)$ \\
						\hline
						RBT & $\Theta(log(n))$ & $\Theta(2log(n))$ \\
						\hline					
					\end{tabular}
					\caption{Inserimento nel dizionario} %\label{tab:sometab}
				\end{table}	
							
				\begin{table}[ht]
					\centering									
					\begin{tabular}{l | l | l}
						
						Algorithm & Average case & Worst Case \\
						\hline
						Hash Table & $\Theta(xxx)$ & $\Theta(xxx)$ \\
						\hline
						BST & $\Theta(log(n))$ & $\Theta(n)$ \\
						\hline
						RBT & $\Theta(log(n))$ & $\Theta(2log(n))$ \\
						\hline					
					\end{tabular}
					\caption{Cancellazione nel dizionario} %\label{tab:sometab}
				\end{table}
						
				\begin{table}[ht]
					\centering									
					\begin{tabular}{l | l | l}
						
						Algorithm & Average case & Worst Case \\
						\hline
						Hash Table & $\Omega(n)$ & $\Theta(n)$ \\
						\hline
						BST & $\Theta(n)$ & $\Theta(n)$ \\
						\hline
						RBT & $\Theta(n)$ & $\Theta(n)$ \\
						\hline					
					\end{tabular}
					\caption{Ricerca nel dizionario} %\label{tab:sometab}
				\end{table}
			 Come si evince dalle tabelle l'algoritmo che presenta il miglior costo computazionale è il \textbf{RBT}.
			 \pagebreak
			 
		\section{Costi computazionali funzioni implementate}
			\subsection{Funzioni implementate da \texttt{Nicolas Farabegoli}}
				In questa sezione vengono analizzati i costi computazionali delle funzioni implementate da Nicolas Farabegoli in maniera più dettagliata (ove possibile).
								
				\texttt{insertWord(NODO** root, NODO* node)} - Il costo computazionale nel caso peggiore della funzione per l'inserimento di un nodo è piuttosto semplice da calcolare in quanto nel caso peggiore bisognerà arrivare alle foglie e quindi percorrere un numero di nodi pari all'altezza dell'albero.Come detto nei paragrafi precedenti, l'altezza di un albero nei RBT nel caso peggiore, è pari a $2log_2(n)$ dove $n$ sono il numero di nodi presenti nell'albero.\par
				
				\texttt{cancWord(NODO** dictionary, char* word)} - La funzione di cancellazione di una parola all'interno del dizionario effettua due chiamate ad altre due funzioni: \texttt{searchWord} per ricercare la parola da cancellare e \texttt{rb\_delete} per eliminare il nodo contenente la parola cercata. La funzione \texttt{searchWord} al suo interno effettua due chiamate ricorsive condizionate: una sul figlio di destra se la parola da cercare ha peso maggiore, oppure a sinistra se la parola ha peso minore. Possiamo quindi impostare un'equazione ricorsiva piuttosto semplice:				
				$$T(n) =
				\bigg \{
				\begin{array}{rl}
				\Theta(1) & n = 1 \\
				T(\frac{n}{2}) & n > 1 \\
				\end{array}
				$$
				L'equazione ricorsiva produce come risultato un costo computazionale pari a $\Theta(log(n)) + k$.
				L'altra funzione presente si occupa semplicemente di rimuovere dall'albero il nodo contente la parola, questa funzione non presente cicli e le uniche operazioni che fa sono qualche rotazione e ricolorazione, quindi operazioni con un costo computazionale pari a $\Theta(k)$.
				Possiamo quindi affermare che il costo computazionale per la funzione \texttt{cancWord(...)} è pari a $\Theta(log(n) + k)$.\par
				
				\texttt{insertDef(NODO* dictionary, char* word, char* def)} - La funzione si occupa di ricercare la parola corretta ed aggiungere la definizione. La funzione al suo interno effettua una chiamata alla funzione \texttt{searchWord(...)} che, come accennato al paragrafo precedente, ha un costo computazionale pari a $\Theta(log(n))$. Le altre operazioni che svolge la funzione sono per l'inserimento della definizione; queste operazioni sono costanti e non dipendono dalla dimensione della struttura dati quindi hanno costo costante. Concludendo, il costo computazionale della funzione è pari a $\Theta(log(n))$.\par
				
				\texttt{searchDef(NODO* dictionary, char* word)} - La funzione si occupa di ricercare la parola passata come argomento e stampare a schermo la sua definizione. la funzione al suo interno richiama \texttt{searchWord(...)}. Quindi il costo computazionale è pressoché identico alla funzione illustrata in precedenza, quindi ha costo computazionale pari a $\Theta(log(n))$.\par
				
				\texttt{saveDictionary(NODO* dictionary, chaar* fileOutput)} - La funzione si occupa di salvare il dizionario in memoria nel PC tramite un file. Al suo interno la funzione, oltre a creare il file di destinazione, chiama la funzione \texttt{printDictionaryFile(...)} che è strutturata così: vengono effettuate due chiamate ricorsive sul figlio di destra e quello di sinistra del sottoalbero. Questo significa che i nodi dell'albero vengono vinistati una ed una sola volta, in questo caso non si ritiene necessario dover risolvere un'equazione ricorsiva in quanto la visita di ogni nodo è costante per ognuno di essi, il costo computazionale di questa funzione è $\Theta(n)$. Come detto sopra, viene creato un file per il salvataggio, supponiamo che questa operazione sia costante; essendo anche l'unica altra funzione presente il costo computazionale finale della funzione è: $\Theta(n)$.\par
				
				\texttt{importDictionary(char* fileInput)} - La funzione si occupa di leggere il file generato dalla funzione \texttt{saveDictionary(...)} e di creare il dizionario. La funzione al suo interno si compone di un ciclo per la lettua del file che termina con la presenza del carattere \texttt{EOF}, funzioni per estrapolare la parola e relativa definizione ( che consideremo con costo computazionale costante ai fini del costo finale della funzione) e la funzione \texttt{insertRBT} che consente di inserire il nodo nell'abero. Quest'ultima funzione presenta costo computazionale pari a $\Theta(2log(n))$ poichè sarà necessario scorrere tutto l'albero fino alle foglie. Siccome questa funzione viene eseguita per ogni nome, il costo computazionale globale della funzione è pari a $\Theta(nlog(n))$.\par 
				
			\subsection{Funzioni implementate da \texttt{Paolo Baldini}}
				...something...
			\pagebreak
		\section{Riflessioni e considerazioni sul progetto}
			\subsection{Difficoltà incontrate}
				Tra le difficoltà più grandi incontrate nel progetto troviamo la creazione delle funzioni relative alla manipolazione della struttura dati principale (RBT); per esempio la funzione per l'inserimento di un nodo nell'albero ha creato non poche difficoltà per il fatto che l'albero non veniva correttamente bilanciato (tutt'ora si potrebbe migliorare ma per motivo di tempo e competenze questo non è stato possibile). La funzione di cacellazione di un nodo (probabilmente per la sua difficoltà intrinseca)  è stata piuttosto complicata da implementare e farla funzionare correttamente. ...Paolo...
				
			\subsection{Considerazioni finali}
				Grazie al progetto abbiamo potuto consolidare le nostre conoscenze circa il funzionamento e le proprietà legate ai RBT. Abbiamo messo in campo le nostre migliori skills per poter affrontare e terminare nel migliore dei modi il compito assegnato e poter confrotarci per migliorare o talvota sistemare/segnalare le problematiche incontrate durante la stesura del codice. Gli strumenti utilizzati per la realizzazione del progetto hanno indubbiamente favorito l'identificazione e di conseguenza la correzione di bugs che impedivano il corretto flusso e coerenza del programma.
				Grazie a questo progetto siamo riusciti ad analizzare tutti gli aspetti (o quasi) del problema e abbiamo sempre cercato le soluzione maggiormente efficienti per implementare le funzioni date.

								
		 
\end{document}